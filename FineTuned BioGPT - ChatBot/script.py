{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [code]\n!pip install peft transformers datasets torch-xla accelerate sacremoses nltk\n\nfrom transformers import BioGptTokenizer , set_seed, default_data_collator, get_linear_schedule_with_warmup ,BioGptForCausalLM\nfrom peft import get_peft_config, get_peft_model, PrefixTuningConfig, TaskType, PeftType\nimport torch , random , torch_xla, torch_xla.core.xla_model as xm\nfrom datasets import load_dataset,DatasetDict, Dataset\nimport os\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\ndevice = xm.xla_device()\nmodel_name_or_path = \"microsoft/BioGPT-Large-PubMedQA\"\ntokenizer_name_or_path = \"microsoft/BioGPT-Large-PubMedQA\"\npeft_config = PrefixTuningConfig(task_type=TaskType.CAUSAL_LM, num_virtual_tokens=10)\n\ndataset_name = \"reginaboateng/cleaned_pubmedqa\"\ncheckpoint_name = f\"{dataset_name}_{model_name_or_path}_{peft_config.peft_type}_{peft_config.task_type}_v1.pt\".replace(\"/\", \"_\")\nquestion_column = \"Question\"\nanswer_column = \"Answer\"\nmax_length = 512\nlr = 1e-5\nnum_epochs = 10\nbatch_size = 32\n\nprint(checkpoint_name)\n\npubmed_qa = load_dataset(\"pubmed_qa\", \"pqa_artificial\")\n\ndata = []\nfor row in pubmed_qa['train']:\n    question = row['question']\n    answer = row['long_answer']\n    data.append({'Question': question, 'Answer': answer})\n\ntrain = data[:100]\nrandom.shuffle(train)\ntrain_mapping = {key: [d[key] for d in train] for key in train[0]}\ntrain_dataset = Dataset.from_dict(train_mapping)\n\nvalidation = data[100:150]\nrandom.shuffle(validation)\nval_mapping = {key: [d[key] for d in validation] for key in validation[0]}\nval_dataset = Dataset.from_dict(val_mapping)\n\ndataset = DatasetDict({'train': train_dataset, 'validation': val_dataset})\ndataset\n\n# data preprocessing\ntokenizer = BioGptTokenizer.from_pretrained(model_name_or_path)\nif tokenizer.pad_token_id is None:\n    tokenizer.pad_token_id = tokenizer.eos_token_id\ntarget_max_length = max([len(tokenizer(answer)[\"input_ids\"]) for answer in dataset['train']['Answer']])\nprint(target_max_length)\n\n\ndef preprocess_function(examples):\n    batch_size = len(examples[question_column])\n    inputs = [f\"{question_column} : {x} Answer : \" for x in examples[question_column]]\n    targets = [str(x) for x in examples[answer_column]]\n    model_inputs = tokenizer(inputs)\n    labels = tokenizer(targets)\n    \n    for i in range(batch_size):\n        sample_input_ids = model_inputs[\"input_ids\"][i]\n        label_input_ids = labels[\"input_ids\"][i] + [tokenizer.pad_token_id]\n\n        model_inputs[\"input_ids\"][i] = sample_input_ids + label_input_ids\n        labels[\"input_ids\"][i] = [-100] * len(sample_input_ids) + label_input_ids\n        model_inputs[\"attention_mask\"][i] = [1] * len(model_inputs[\"input_ids\"][i])\n\n    for i in range(batch_size):\n        sample_input_ids = model_inputs[\"input_ids\"][i]\n        label_input_ids = labels[\"input_ids\"][i]\n        model_inputs[\"input_ids\"][i] = [tokenizer.pad_token_id] * (\n            max_length - len(sample_input_ids)\n        ) + sample_input_ids\n        model_inputs[\"attention_mask\"][i] = [0] * (max_length - len(sample_input_ids)) + model_inputs[\n            \"attention_mask\"\n        ][i]\n        labels[\"input_ids\"][i] = [-100] * (max_length - len(sample_input_ids)) + label_input_ids\n        model_inputs[\"input_ids\"][i] = torch.tensor(model_inputs[\"input_ids\"][i][:max_length])\n        model_inputs[\"attention_mask\"][i] = torch.tensor(model_inputs[\"attention_mask\"][i][:max_length])\n        labels[\"input_ids\"][i] = torch.tensor(labels[\"input_ids\"][i][:max_length])\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\n\nprocessed_datasets = dataset.map(\n    preprocess_function,\n    batched=True,\n    num_proc=1,\n    remove_columns=dataset[\"train\"].column_names,\n    load_from_cache_file=False,\n    desc=\"Running tokenizer on dataset\",\n)\n\ntrain_dataset = processed_datasets[\"train\"]\neval_dataset = processed_datasets[\"train\"]\n\n\ntrain_dataloader = DataLoader(\n    train_dataset, shuffle=True, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True\n)\neval_dataloader = DataLoader(eval_dataset, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True)\n\nnext(iter(train_dataloader))\n\nmodel = BioGptForCausalLM.from_pretrained(model_name_or_path)\n\n# model\n# optimizer and lr scheduler\noptimizer = torch.optim.AdamW(model.parameters(), lr=lr)\nlr_scheduler = get_linear_schedule_with_warmup(\n    optimizer=optimizer,\n    num_warmup_steps=0,\n    num_training_steps=(len(train_dataloader) * num_epochs),\n)\n\n# training and evaluation\nmodel = model.to(device)\n\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss = 0\n    for step, batch in enumerate(tqdm(train_dataloader)):\n        batch = {k: v.to(device) for k, v in batch.items()}\n        outputs = model(**batch)\n        loss = outputs.loss\n        total_loss += loss.detach().float()\n        loss.backward()\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n\n    model.eval()\n    eval_loss = 0\n    eval_preds = []\n    for step, batch in enumerate(tqdm(eval_dataloader)):\n        batch = {k: v.to(device) for k, v in batch.items()}\n        with torch.no_grad():\n            outputs = model(**batch)\n        loss = outputs.loss\n        eval_loss += loss.detach().float()\n        eval_preds.extend(\n            tokenizer.batch_decode(torch.argmax(outputs.logits, -1).detach().cpu().numpy(), skip_special_tokens=True)\n        )\n\n    eval_epoch_loss = eval_loss / len(eval_dataloader)\n    eval_ppl = torch.exp(eval_epoch_loss)\n    train_epoch_loss = total_loss / len(train_dataloader)\n    train_ppl = torch.exp(train_epoch_loss)\n    print(f\"{epoch=}: {train_ppl=} {train_epoch_loss=} {eval_ppl=} {eval_epoch_loss=}\")\n\nmodel.eval()\ni = 16\ninputs = tokenizer(f'{question_column} : {dataset[\"validation\"][i][\"Question\"]} Answer : ', return_tensors=\"pt\")\nprint(dataset[\"validation\"][i][\"Question\"])\nprint(inputs)\n\nwith torch.no_grad():\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n    outputs = model.generate(\n        input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], max_new_tokens=10)\n    print(outputs)\n    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True))\n\n# saving model\npeft_model_id = f\"{model_name_or_path}_{peft_config.peft_type}_{peft_config.task_type}\"\nmodel.save_pretrained(peft_model_id)","metadata":{"_uuid":"a81d961d-45f4-4142-a452-1efd662ee87b","_cell_guid":"6200f882-5e7a-4b43-b038-b26526eb22c4","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}